{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gts = pd.read_csv(\"./train/train_gts.csv\")\n",
    "meta = pd.read_csv(\"./train/train_meta.csv\")\n",
    "\n",
    "df = meta.merge(gts, on='record_name', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./test/test_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_record(record_name, is_train):\n",
    "    if is_train:\n",
    "        with open(f\"./train/{record_name}.npy\", \"rb\") as f:\n",
    "            data  = np.load(f, allow_pickle=True)\n",
    "    else:\n",
    "        with open(f\"./test/{record_name}.npy\", \"rb\") as f:\n",
    "            data  = np.load(f, allow_pickle=True)\n",
    "    return data\n",
    "\n",
    "def print_beat(x, y, ecg_channel):\n",
    "    fig, ax = plt.subplots()  # Create a figure containing a single axes.\n",
    "    ax.set_title(f\"ECG channel {ecg_channel}\")\n",
    "    ax.plot(x, y)  # Plot some data on the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "class Transformator():\n",
    "    \"\"\" Класс-пайплайн от датафрейма train.gts до трансформированных данных ЭКГ и аннотаций для тренировки модели.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,args, transformation_func, is_train):\n",
    "        self.is_train = is_train\n",
    "        self.args = args\n",
    "        self.transformation_func = transformation_func\n",
    "    \n",
    "    def run_pipeline(self, df):\n",
    "        \n",
    "        result_df = self.pipeline_ecg(self.transformation_func, df)\n",
    "        return result_df\n",
    "\n",
    "    # transformation func - function that transforms record to any \n",
    "    def pipeline_ecg(self, transformation_func, df):\n",
    "\n",
    "        result_df = df.copy()\n",
    "        record_names = result_df['record_name'].to_list()\n",
    "        result_df['correct_transformation'] = True\n",
    "        path = \"\"\n",
    "        if self.is_train: \n",
    "            if(not os.path.exists(\"./transformed_train/\")):\n",
    "                os.mkdir(\"./transformed_train/\")\n",
    "            path = \"./transformed_train/\"\n",
    "        else:\n",
    "            if(not os.path.exists(\"./transformed_test/\")):\n",
    "                os.mkdir(\"./transformed_test/\")\n",
    "            path = \"./transformed_test/\"\n",
    "\n",
    "        new_names_column = [] \n",
    "        for record_name in tqdm(record_names):\n",
    "            transformed = transformation_func(record_name, **self.args)\n",
    "            # transformed - shape [12, 9] if preprocessing_with beats\n",
    "\n",
    "            new_names = []\n",
    "            for i in range(transformed.shape[1]):\n",
    "                name = f\"{record_name}_n{i}\"\n",
    "                new_names.append(name)\n",
    "                np.save(path+name+\".npy\", transformed[:, i])\n",
    "            new_names_column.append(new_names)\n",
    "        result_df[\"new_name\"] = new_names_column\n",
    "\n",
    "        result_df = result_df.explode(\"new_name\", ignore_index=True) \n",
    "        return result_df \n",
    "\n",
    "    def check_correctnes(self, df):\n",
    "        test_df = df.iloc[:3]\n",
    "        self.run_pipeline(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_with_beats import PipelineBeatExtraction\n",
    "pipeline = PipelineBeatExtraction(prefix=\"./train\", noise_level=2)\n",
    "pipeline_test = PipelineBeatExtraction(prefix=\"./test\", noise_level=2)\n",
    "pipeline_func_test = pipeline_test.run_pipeline\n",
    "pipeline_func = pipeline.run_pipeline\n",
    "\n",
    "transformator = Transformator({}, pipeline_func, is_train=True )\n",
    "transformator_test = Transformator({}, pipeline_func_test, is_train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на правильность работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b965c37d9847f89c0c0b498856f6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformator.check_correctnes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea84e8b05e604fa98e9f489b3350cf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformator_test.check_correctnes(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f955a141f3794383987f76b6d537f804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the process_dataframe function\n",
    "process_num = 12\n",
    "\n",
    "def process_dataframe(df):\n",
    "    result_df = transformator.pipeline_ecg(transformator.transformation_func, df)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Split your data into multiple dataframes\n",
    "dfs = np.array_split(df, process_num) \n",
    "\n",
    "\n",
    "# Create a pool of worker processes\n",
    "pool = multiprocessing.Pool(processes=process_num)\n",
    "\n",
    "result_dfs = []\n",
    "# Run the process_dataframe function in parallel for each dataframe\n",
    "#result_dfs = pool.map(process_dataframe, dfs)\n",
    "for result_df in tqdm(pool.imap(process_dataframe, dfs)):\n",
    "    result_dfs.append(result_df)\n",
    "\n",
    "# Close the pool of worker processes\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "result_df = pd.concat(result_dfs)\n",
    "result_df.to_csv(\"transformed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb8209eb7954f13aed4e587602c0dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_dataframe(df):\n",
    "    result_df = transformator_test.pipeline_ecg(transformator_test.transformation_func, df)\n",
    "    return result_df\n",
    "\n",
    "# Split your data into multiple dataframes\n",
    "test_dfs = np.array_split(df_test, process_num) \n",
    "\n",
    "\n",
    "# Create a pool of worker processes\n",
    "pool = multiprocessing.Pool(processes=process_num)\n",
    "\n",
    "result_test_dfs = []\n",
    "# Run the process_dataframe function in parallel for each dataframe\n",
    "#result_dfs = pool.map(process_dataframe, dfs)\n",
    "for result_test_df in tqdm(pool.imap(process_dataframe, test_dfs)):\n",
    "    result_test_dfs.append(result_test_df)\n",
    "\n",
    "# Close the pool of worker processes\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "result_test_df = pd.concat(result_test_dfs)\n",
    "result_test_df.to_csv(\"transformed_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_annotations(df):\n",
    "    annotations = df[['new_name', 'strat_fold','myocard']]   \n",
    "\n",
    "    val_annotations = annotations[annotations['strat_fold'].isin([8, 9, 10])]\n",
    "    train_annotations = annotations[annotations['strat_fold'].isin(range(1,8))]\n",
    "    \n",
    "    # shuffle rows\n",
    "    val_annotations = val_annotations.sample(frac=1)\n",
    "    train_annotations = train_annotations.sample(frac=1)\n",
    "    \n",
    "    val_annotations.to_csv('./val_annotations.csv', index=False)\n",
    "    train_annotations.to_csv('./train_annotations.csv', index=False)\n",
    "    \n",
    "    print(\"number of validation samples:\",len(val_annotations) )\n",
    "    print(\"number of train samples:\",len(train_annotations))\n",
    "\n",
    "    print(\"validation percentage:\", len(val_annotations) / len(annotations))\n",
    "    print(\"train percentage:\",len(train_annotations) / len(annotations))\n",
    "\n",
    "    print(\"VALIDATION myocard percentage\", val_annotations[\"myocard\"].value_counts(normalize=True))\n",
    "    print(\"TTRAIN myocard percentage\", train_annotations[\"myocard\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, fraction=1):\n",
    "        \"\"\"\n",
    "        function to downsample majority class.\n",
    "        \"\"\"\n",
    "        # Separate the majority and minority classes\n",
    "        majority_class = df[df['myocard'] == 0]\n",
    "        minority_class = df[df['myocard'] == 1]  \n",
    "        \n",
    "        # Calculate the number of samples needed from the majority class\n",
    "        num_samples = len(minority_class)\n",
    "        \n",
    "        downsampled_majority = majority_class.sample(n=int(num_samples*fraction)) \n",
    "        \n",
    "        downsampled_df = pd.concat([downsampled_majority, minority_class], axis=0)\n",
    "        \n",
    "        return downsampled_df\n",
    "\n",
    "def oversample(df):\n",
    "    \"\"\"\n",
    "    Function to oversample the minority class by copying rows.\n",
    "    \"\"\"\n",
    "    # Separate the majority and minority classes\n",
    "    majority_class = df[df['myocard'] == 0]\n",
    "    minority_class = df[df['myocard'] == 1]  \n",
    "    \n",
    "    # Calculate the number of samples needed from the majority class\n",
    "    num_samples = len(majority_class) - len(minority_class)\n",
    "    \n",
    "    # Oversample by copying rows from the minority class\n",
    "    oversampled_minority = minority_class.sample(n=num_samples, replace=True)\n",
    "    \n",
    "    oversampled_df = pd.concat([majority_class, oversampled_minority], axis=0)\n",
    "    \n",
    "    return oversampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = downsample(result_df, fraction=1)\n",
    "#oversampled_df = oversample(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of validation samples: 2795\n",
      "number of train samples: 5851\n",
      "validation percentage: 0.3232708767059912\n",
      "train percentage: 0.6767291232940088\n",
      "VALIDATION myocard percentage 1    0.514132\n",
      "0    0.485868\n",
      "Name: myocard, dtype: float64\n",
      "TTRAIN myocard percentage 0    0.506751\n",
      "1    0.493249\n",
      "Name: myocard, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pipeline_annotations(downsampled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
