{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gts = pd.read_csv(\"../task_final/train/train_gts.csv\")\n",
    "gts_final = gts = pd.read_csv(\"../task_final/train/train_gts_final.csv\")\n",
    "meta = pd.read_csv(\"../task_final/train/train_meta.csv\")\n",
    "\n",
    "df = meta.merge(gts_final, on='record_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>record_name</th>\n",
       "      <th>перегородочный</th>\n",
       "      <th>передний</th>\n",
       "      <th>боковой</th>\n",
       "      <th>передне-боковой</th>\n",
       "      <th>передне-перегородочный</th>\n",
       "      <th>нижний</th>\n",
       "      <th>норма</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18792.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>00009_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13619.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00034_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11315.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>00043_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18153.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>00052_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16063.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>00057_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>12488.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21784_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>10162.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21795_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>11197.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21825_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>11905.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21831_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>20703.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21834_hr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2101 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patient_id    age  sex  height  weight record_name  перегородочный  \\\n",
       "0        18792.0   55.0    0     NaN    70.0    00009_hr               0   \n",
       "1        13619.0   56.0    0     NaN     NaN    00034_hr               0   \n",
       "2        11315.0   25.0    1     NaN    63.0    00043_hr               0   \n",
       "3        18153.0   35.0    0     NaN    82.0    00052_hr               0   \n",
       "4        16063.0   26.0    0     NaN    93.0    00057_hr               0   \n",
       "...          ...    ...  ...     ...     ...         ...             ...   \n",
       "2096     12488.0   66.0    1     NaN     NaN    21784_hr               0   \n",
       "2097     10162.0   68.0    0     NaN     NaN    21795_hr               0   \n",
       "2098     11197.0   59.0    0     NaN     NaN    21825_hr               0   \n",
       "2099     11905.0   55.0    1     NaN     NaN    21831_hr               0   \n",
       "2100     20703.0  300.0    0     NaN     NaN    21834_hr               0   \n",
       "\n",
       "      передний  боковой  передне-боковой  передне-перегородочный  нижний  \\\n",
       "0            0        0                0                       0       0   \n",
       "1            0        0                0                       0       0   \n",
       "2            0        0                0                       0       0   \n",
       "3            0        0                0                       0       0   \n",
       "4            0        0                0                       0       0   \n",
       "...        ...      ...              ...                     ...     ...   \n",
       "2096         0        0                0                       0       0   \n",
       "2097         0        0                0                       0       0   \n",
       "2098         0        0                0                       0       0   \n",
       "2099         0        0                0                       0       0   \n",
       "2100         0        0                0                       0       1   \n",
       "\n",
       "      норма  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "2096      1  \n",
       "2097      1  \n",
       "2098      1  \n",
       "2099      1  \n",
       "2100      0  \n",
       "\n",
       "[2101 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = np.argmax(df[[\"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\", \"норма\"]].to_numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../task_final/test/test_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_record(record_name, is_train):\n",
    "    if is_train:\n",
    "        with open(f\"../task_final/train/{record_name}.npy\", \"rb\") as f:\n",
    "            data  = np.load(f, allow_pickle=True)\n",
    "    else:\n",
    "        with open(f\"../task_final/test/{record_name}.npy\", \"rb\") as f:\n",
    "            data  = np.load(f, allow_pickle=True)\n",
    "    return data\n",
    "\n",
    "def print_beat(x, y, ecg_channel):\n",
    "    fig, ax = plt.subplots()  # Create a figure containing a single axes.\n",
    "    ax.set_title(f\"ECG channel {ecg_channel}\")\n",
    "    ax.plot(x, y)  # Plot some data on the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "class Transformator():\n",
    "    \"\"\" Класс-пайплайн от датафрейма train.gts до трансформированных данных ЭКГ.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,args, transformation_func, is_train):\n",
    "        self.is_train = is_train\n",
    "        self.args = args\n",
    "        self.transformation_func = transformation_func\n",
    "    \n",
    "    def run_pipeline(self, df):\n",
    "        \n",
    "        result_df = self.pipeline_ecg(self.transformation_func, df)\n",
    "        return result_df\n",
    "\n",
    "    # transformation func - function that transforms record to any \n",
    "    def pipeline_ecg(self, transformation_func, df):\n",
    "\n",
    "        result_df = df.copy()\n",
    "        record_names = result_df['record_name'].to_list()\n",
    "        result_df['correct_transformation'] = True\n",
    "        path = \"\"\n",
    "        if self.is_train: \n",
    "            if(not os.path.exists(\"./transformed_train/\")):\n",
    "                os.mkdir(\"./transformed_train/\")\n",
    "            path = \"./transformed_train/\"\n",
    "        else:\n",
    "            if(not os.path.exists(\"./transformed_test/\")):\n",
    "                os.mkdir(\"./transformed_test/\")\n",
    "            path = \"./transformed_test/\"\n",
    "\n",
    "        new_names_column = [] \n",
    "        for record_name in tqdm(record_names):\n",
    "            transformed = transformation_func(record_name, **self.args)\n",
    "            # transformed - shape [12, 9] if preprocessing_with beats\n",
    "\n",
    "            new_names = []\n",
    "            for i in range(transformed.shape[1]):\n",
    "                name = f\"{record_name}_n{i}\"\n",
    "                new_names.append(name)\n",
    "                np.save(path+name+\".npy\", transformed[:, i])\n",
    "            new_names_column.append(new_names)\n",
    "        result_df[\"new_name\"] = new_names_column\n",
    "\n",
    "        result_df = result_df.explode(\"new_name\", ignore_index=True) \n",
    "        return result_df \n",
    "\n",
    "    def check_correctnes(self, df):\n",
    "        test_df = df.iloc[:3]\n",
    "        self.run_pipeline(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 10,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_with_beats import PipelineBeatExtraction\n",
    "pipeline = PipelineBeatExtraction(prefix=\"../task_final/train\", noise_level=2)\n",
    "pipeline_test = PipelineBeatExtraction(prefix=\"../task_final/test\", noise_level=2)\n",
    "pipeline_func_test = pipeline_test.run_pipeline\n",
    "pipeline_func = pipeline.run_pipeline\n",
    "\n",
    "transformator = Transformator({}, pipeline_func, is_train=True )\n",
    "transformator_test = Transformator({}, pipeline_func_test, is_train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на правильность работы."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 11,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "363ce4f20ab047968c5c5fc60e46a17e",
=======
       "model_id": "e8dcc06dab5941bdb821b49e91a730f7",
>>>>>>> refs/remotes/origin/main
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformator.check_correctnes(df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 12,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "9b8fb85d1b52436bb26db95244906a46",
=======
       "model_id": "38f0cf38054e42f8a39540e16e7c0737",
>>>>>>> refs/remotes/origin/main
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformator_test.check_correctnes(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск препроцессинга для тренировочных данных в многопроцессном режиме."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73b50a91d194986bdb7e67eb61f04d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/D/conda/envs/open-mmlab/lib/python3.8/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/mnt/D/conda/envs/open-mmlab/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   1 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed: 21.3min remaining: 63.9min\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed: 21.3min remaining: 35.6min\n",
      "[Parallel(n_jobs=8)]: Done   4 out of   8 | elapsed: 21.5min remaining: 21.5min\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed: 21.6min remaining: 13.0min\n",
      "[Parallel(n_jobs=8)]: Done   6 out of   8 | elapsed: 21.6min remaining:  7.2min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed: 21.7min finished\n"
>>>>>>> refs/remotes/origin/main
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_dataframe(df):\n",
    "    result_df = transformator.pipeline_ecg(transformator.transformation_func, df)\n",
    "    return result_df\n",
    "\n",
    "process_num = 8\n",
    "\n",
<<<<<<< HEAD
    "# Split your data into multiple dataframes\n",
    "dfs = np.array_split(df, process_num) \n",
=======
    "result_test_dfs = Parallel(\n",
    "    n_jobs=process_num,\n",
    "    verbose=100\n",
    ")(\n",
    "    delayed(process_dataframe)(x) for x in np.array_split(df, process_num)\n",
    ")\n",
>>>>>>> refs/remotes/origin/main
    "\n",
    "result_df = pd.concat(result_test_dfs)\n",
    "result_df.to_csv(\"transformed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd0e3c24748467eaa571b5ecd403a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the process_dataframe function\n",
    "# process_num = 12\n",
    "\n",
    "# def process_dataframe(df):\n",
    "#     result_df = transformator.pipeline_ecg(transformator.transformation_func, df)\n",
    "#     return result_df\n",
    "\n",
    "\n",
    "# # Split your data into multiple dataframes\n",
    "# dfs = np.array_split(df, process_num) \n",
    "\n",
    "\n",
    "# # Create a pool of worker processes\n",
    "# pool = multiprocessing.Pool(processes=process_num)\n",
    "\n",
    "# result_dfs = []\n",
    "# # Run the process_dataframe function in parallel for each dataframe\n",
    "# for result_df in tqdm(pool.imap(process_dataframe, dfs)):\n",
    "#     result_dfs.append(result_df)\n",
    "\n",
    "# # Close the pool of worker processes\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# result_df = pd.concat(result_dfs)\n",
    "# result_df.to_csv(\"transformed_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск препроцессинга для тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   1 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:  4.9min remaining: 14.8min\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:  4.9min remaining:  8.2min\n",
      "[Parallel(n_jobs=8)]: Done   4 out of   8 | elapsed:  4.9min remaining:  4.9min\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:  5.0min remaining:  3.0min\n",
      "[Parallel(n_jobs=8)]: Done   6 out of   8 | elapsed:  5.0min remaining:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:  5.1min finished\n"
     ]
>>>>>>> refs/remotes/origin/main
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_dataframe(df):\n",
    "    result_df = transformator_test.pipeline_ecg(transformator_test.transformation_func, df)\n",
    "    return result_df\n",
    "\n",
    "process_num = 8\n",
    "\n",
    "result_test_dfs = Parallel(\n",
    "    n_jobs=process_num,\n",
    "    verbose=100\n",
    ")(\n",
    "    delayed(process_dataframe)(x) for x in np.array_split(df_test, process_num)\n",
    ")\n",
    "\n",
    "result_test_df = pd.concat(result_test_dfs)\n",
    "result_test_df.to_csv(\"transformed_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_dataframe(df):\n",
    "#     result_df = transformator_test.pipeline_ecg(transformator_test.transformation_func, df)\n",
    "#     return result_df\n",
    "\n",
    "# # Split your data into multiple dataframes\n",
    "# test_dfs = np.array_split(df_test, process_num) \n",
    "\n",
    "\n",
    "# # Create a pool of worker processes\n",
    "# pool = multiprocessing.Pool(processes=process_num)\n",
    "\n",
    "# result_test_dfs = []\n",
    "# # Run the process_dataframe function in parallel for each dataframe\n",
    "# #result_dfs = pool.map(process_dataframe, dfs)\n",
    "# for result_test_df in tqdm(pool.imap(process_dataframe, test_dfs)):\n",
    "#     result_test_dfs.append(result_test_df)\n",
    "\n",
    "# # Close the pool of worker processes\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# result_test_df = pd.concat(result_test_dfs)\n",
    "# result_test_df.to_csv(\"transformed_test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание аннотаций для тренировки модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_annotations(df):\n",
    "    annotations = df[['new_name', 'target', \"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\", \"норма\"]]   \n",
    "    cv = StratifiedGroupKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    train_idxs, test_idxs = next(cv.split(annotations[\"new_name\"], annotations[\"target\"], df[\"record_name\"]))\n",
    "    \n",
    "\n",
    "    # names_train, names_val, targets_train, targets_val = train_test_split(annotations[\"new_name\"], \n",
    "    #                                                                       annotations[\"target\"], stratify=annotations[\"target\"],\n",
    "    #                                                                       test_size=0.3)\n",
    "\n",
    "    val_annotations = annotations.iloc[test_idxs]\n",
    "    train_annotations = annotations.iloc[train_idxs]\n",
    "    \n",
    "    # shuffle rows\n",
    "    val_annotations = val_annotations.sample(frac=1)\n",
    "    train_annotations = train_annotations.sample(frac=1)\n",
    "    \n",
    "    val_annotations.to_csv('./val_annotations.csv', index=False)\n",
    "    train_annotations.to_csv('./train_annotations.csv', index=False)\n",
    "    \n",
    "    print(\"number of validation samples:\",len(val_annotations) )\n",
    "    print(\"number of train samples:\",len(train_annotations))\n",
    "\n",
    "    print(\"validation percentage:\", len(val_annotations) / len(annotations))\n",
    "    print(\"train percentage:\",len(train_annotations) / len(annotations))\n",
    "\n",
    "    print(\"VALIDATION myocard percentage\", val_annotations[\"target\"].value_counts(normalize=True))\n",
    "    print(\"TTRAIN myocard percentage\", train_annotations[\"target\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 163,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, fraction=1):\n",
    "        \"\"\"\n",
    "        function to downsample majority class.\n",
    "        \"\"\"\n",
    "        # Separate the majority and minority classes\n",
    "        majority_class = df[df['норма'] == 1]\n",
    "        minority_class = df[df['норма'] != 1]  \n",
    "        \n",
    "        # Calculate the number of samples needed from the majority class\n",
    "        num_samples = len(minority_class)\n",
    "        \n",
    "        downsampled_majority = majority_class.sample(frac=fraction, replace=True) \n",
    "        \n",
    "        downsampled_df = pd.concat([downsampled_majority, minority_class], axis=0)\n",
    "        \n",
    "        return downsampled_df\n",
    "\n",
    "def oversample(df):\n",
    "    \"\"\"\n",
    "    Function to oversample the minority class by copying rows.\n",
    "    \"\"\"\n",
    "    minority_classes = [\"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \"передне-перегородочный\", \"нижний\"]\n",
    "    majority_class = df[df['норма'] == 1]\n",
    "\n",
    "    dfs = [majority_class]\n",
    "    for minority_class in minority_classes:\n",
    "        # Separate the majority and minority classes\n",
    "        minority_df = df[df[minority_class] == 1]\n",
    "        \n",
    "        # Calculate the number of samples needed from the majority class\n",
    "        num_samples = len(majority_class)\n",
    "        frac = num_samples / len(minority_df)\n",
    "        \n",
    "        # Oversample by copying rows from the minority class\n",
    "        print(minority_class, num_samples)\n",
    "    \n",
    "        oversampled_minority = minority_df.sample(frac=frac, replace=True)\n",
    "        dfs.append(oversampled_minority.copy())\n",
    "        \n",
    "    oversampled_df = pd.concat(dfs, axis=0)\n",
    "        \n",
    "    return oversampled_df\n",
    "\n",
    "\n",
    "def prepare_balance(df, column=\"передне-боковой\"):\n",
    "    \"\"\"\n",
    "    Function to oversample the minority class by copying rows.\n",
    "    \"\"\"\n",
    "    minority_classes = [x for x in [\"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \"передне-перегородочный\", \"нижний\", \"норма\"]\n",
    "                        if x != column]\n",
    "    majority_class = df[df[column] == 1]\n",
    "    num_samples = round(len(majority_class) / 6)\n",
    "\n",
    "    dfs = [majority_class]\n",
    "    for minority_class in minority_classes:\n",
    "        # Separate the majority and minority classes\n",
    "        minority_df = df[df[minority_class] == 1]\n",
    "        \n",
    "        # Calculate the number of samples needed from the majority class\n",
    "        frac = num_samples / len(minority_df)\n",
    "        \n",
    "        # Oversample by copying rows from the minority class\n",
    "        print(minority_class, frac, len(minority_df))\n",
    "        oversampled_minority = minority_df.sample(frac=frac, replace=True)\n",
    "        print(len(oversampled_minority))\n",
    "        dfs.append(oversampled_minority.copy())\n",
    "        \n",
    "    oversampled_df = pd.concat(dfs, axis=0)\n",
    "        \n",
    "    return oversampled_df\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
=======
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
>>>>>>> refs/remotes/origin/main
   "outputs": [],
   "source": [
    "targets = ['0000100', '0000001', '0100000', '0000110', '0100010', '0010000',\n",
    " '1000010', '0001100', '0101010', '0001000', '0001010', '0000010', '1000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "6    0.802475\n",
       "5    0.075678\n",
       "4    0.055212\n",
       "0    0.027606\n",
       "1    0.026178\n",
       "3    0.011899\n",
       "2    0.000952\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(\"transformed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.merge(df[[\"record_name\", \"target\", \"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\", \"норма\"]], on=\"record_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = downsample(result_df, fraction=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "number of validation samples: 10825\n",
      "number of train samples: 10815\n",
      "validation percentage: 0.5002310536044362\n",
      "train percentage: 0.4997689463955638\n",
      "VALIDATION myocard percentage 0    0.800185\n",
      "1    0.199815\n",
      "Name: myocard, dtype: float64\n",
      "TTRAIN myocard percentage 0    0.798798\n",
      "1    0.201202\n",
      "Name: myocard, dtype: float64\n"
=======
      "перегородочный 10381\n",
      "передний 10381\n",
      "боковой 10381\n",
      "передне-боковой 10381\n",
      "передне-перегородочный 10381\n",
      "нижний 10381\n"
>>>>>>> refs/remotes/origin/main
     ]
    }
   ],
   "source": [
    "oversampled_df = oversample(downsampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = prepare_balance(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                        104136316\n",
       "patient_id                                                      770018759.0\n",
       "age                                                               5210536.0\n",
       "sex                                                                   30915\n",
       "height                                                            4319677.0\n",
       "weight                                                            1819307.0\n",
       "record_name               06250_hr02035_hr12814_hr12161_hr02767_hr18846_...\n",
       "myocard                                                               62286\n",
       "correct_transformation                                                72667\n",
       "new_name                  06250_hr_n502035_hr_n812814_hr_n712161_hr_n302...\n",
       "target                                                               208908\n",
       "перегородочный                                                        10969\n",
       "передний                                                              11589\n",
       "боковой                                                               10381\n",
       "передне-боковой                                                       11363\n",
       "передне-перегородочный                                                13112\n",
       "нижний                                                                21469\n",
       "норма                                                                 10381\n",
       "dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of validation samples: 14132\n",
      "number of train samples: 58535\n",
      "validation percentage: 0.19447617212765078\n",
      "train percentage: 0.8055238278723492\n",
      "VALIDATION myocard percentage target\n",
      "1    0.215539\n",
      "0    0.199901\n",
      "3    0.171950\n",
      "4    0.158435\n",
      "6    0.138975\n",
      "5    0.115200\n",
      "Name: proportion, dtype: float64\n",
      "TTRAIN myocard percentage target\n",
      "2    0.177347\n",
      "4    0.153395\n",
      "1    0.145947\n",
      "6    0.143794\n",
      "3    0.139968\n",
      "0    0.139130\n",
      "5    0.100419\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pipeline_annotations(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34817276,  2.87912088, 10.69387755,  6.23809524,  5.75824176,\n",
       "       74.85714286,  0.29942857])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "compute_class_weight(class_weight='balanced', classes=downsampled_df.target.unique(), y=downsampled_df.target.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.90062112,\n",
       " 4.95885714,\n",
       " 2.10121065,\n",
       " 2.0323185,\n",
       " 1.01699285,\n",
       " 0.75271056,\n",
       " 0.28571429]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([0.28571429,  0.75271056,  1.01699285,  2.0323185 ,  2.10121065,\n",
    "       53.90062112,  4.95885714], key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
