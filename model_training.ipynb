{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from torchmetrics.classification import F1Score, BinaryF1Score\n",
    "from torchmetrics.classification.accuracy import Accuracy, BinaryAccuracy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from resnet1d import ResNet1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\n",
    "    \"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/ConvNets/\"\n",
    "\n",
    "\n",
    "# Function for setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест на работоспособность модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test beat shape (1, 12, 500)\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4971, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_beat = np.load('./transformed_train/00269_hr_n1.npy')\n",
    "kernel_size = 16\n",
    "stride = 2\n",
    "n_block = 48\n",
    "downsample_gap = 6\n",
    "increasefilter_gap = 12\n",
    "model = ResNet1D(\n",
    "    in_channels=12, \n",
    "    base_filters=16, # 64 for ResNet1D, 352 for ResNeXt1D\n",
    "    kernel_size=16, \n",
    "    stride=2, \n",
    "    groups=1, \n",
    "    n_block=12, \n",
    "    n_classes=1) \n",
    "\n",
    "\n",
    "test_beat = test_beat.reshape((1,12,-1))\n",
    "test_y = torch.tensor([[1.]])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(\"test beat shape\", test_beat.shape)\n",
    "res= model(torch.from_numpy(test_beat).float())\n",
    "print(res.shape)\n",
    "criterion(res, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetECG(Dataset):\n",
    "    def __init__(self, annotations_file, signals_dir):\n",
    "        \"\"\"\n",
    "        annotantions_file - path to the annotations dataframe. \n",
    "                            First column should be name of the record, second - strat_fold then labels \n",
    "        \n",
    "        signals_dir - path to the directory with transformed signals\n",
    "        \"\"\"\n",
    "        self.signals_labels = pd.read_csv(annotations_file)\n",
    "        self.signals_labels = self.signals_labels[self.signals_labels[\"норма\"] != 1]\n",
    "        self.signals_dir = signals_dir \n",
    "        self.targets = ['0000100', '0000001', '0100000', '0000110', '0100010', '0010000',\n",
    "                        '1000010', '0001100', '0101010', '0001000', '0001010', '0000010', '1000000']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signals_path = os.path.join(self.signals_dir, self.signals_labels.iloc[idx, 0]+ \".npy\")\n",
    "        signal = np.load(signals_path).astype(np.float32)        \n",
    "\n",
    "        # iloc[idx, 2:] 2 is because first column is a record name\n",
    "        # label = \"\".join(self.signals_labels.iloc[idx, 2:].values.astype(str).tolist())\n",
    "        labels = torch.from_numpy(self.signals_labels.iloc[idx, 2:].values.astype(int)).float()\n",
    "        # label = self.targets.index(label)\n",
    "        # encoded_label = torch.zeros(13)\n",
    "        # encoded_label[label] = 1\n",
    "        return signal, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetECG(\"./train_annotations.csv\", \"transformed_train\")\n",
    "val_dataset = DatasetECG(\"./val_annotations.csv\", \"transformed_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05890683, -0.06293108, -0.06411544, ..., -0.13181776,\n",
       "         -0.09761459, -0.07708762],\n",
       "        [-0.02450616, -0.01948033, -0.02018399, ..., -0.02210358,\n",
       "         -0.01811795, -0.02564406],\n",
       "        [-0.01718437, -0.01770631, -0.01826322, ...,  0.01021589,\n",
       "          0.01456764,  0.01620204],\n",
       "        ...,\n",
       "        [-0.05783983, -0.05812125, -0.05830753, ..., -0.18112521,\n",
       "         -0.16387962, -0.1487573 ],\n",
       "        [-0.04680269, -0.04760379, -0.0453967 , ..., -0.12847553,\n",
       "         -0.12293577, -0.12550604],\n",
       "        [-0.03656435, -0.03702431, -0.03779095, ..., -0.08415375,\n",
       "         -0.0792128 , -0.08007737]], dtype=float32),\n",
       " tensor([0., 0., 0., 0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) #, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) #, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lightning_ResNet1D(pl.LightningModule):\n",
    "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams, model_class=ResNet1D, task=\"MULTILABEL\"):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams - Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
    "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = model_class(**model_hparams)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.BCELoss()\n",
    "        self.train_score = F1Score(task=task, num_labels=model_hparams[\"n_classes\"], num_classes=model_hparams[\"n_classes\"])\n",
    "        self.val_score = F1Score(task=task, num_labels=model_hparams[\"n_classes\"], num_classes=model_hparams[\"n_classes\"])\n",
    "        self.test_score = F1Score(task=task, num_labels=model_hparams[\"n_classes\"], num_classes=model_hparams[\"n_classes\"])\n",
    "        self.val_acc = Accuracy(task=task, num_labels=model_hparams[\"n_classes\"], num_classes=model_hparams[\"n_classes\"])\n",
    "        self.train_acc = Accuracy(task=task, num_labels=model_hparams[\"n_classes\"], num_classes=model_hparams[\"n_classes\"])\n",
    "        # Example input for visualizing the graph in Tensorboard\n",
    "        self.example_input_array = torch.zeros((1, 12, 500), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # Forward function that is run when visualizing the graph\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We will support Adam or SGD as optimizers.\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            # AdamW is Adam with a correct implementation of weight decay (see here\n",
    "            # for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
    "            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        elif self.hparams.optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n",
    "\n",
    "        # We will reduce the learning rate by 0.1 every milestone\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[35,65, 115, 150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        self.model.train()\n",
    "        imgs, labels = batch\n",
    "        labels = np.squeeze(labels)\n",
    "        preds = np.squeeze(self.model(imgs))\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        if len(preds.shape) < 2:\n",
    "            preds = preds.view(1, -1)\n",
    "            labels = labels.view(1, -1)\n",
    "        # print(preds.shape, labels.shape)\n",
    "        if False:\n",
    "            preds = preds.argmax(axis=1)\n",
    "            labels = labels.to(torch.int).argmax(axis=1)\n",
    "            \n",
    "        self.train_acc(preds, labels.to(torch.int))\n",
    "\n",
    "        self.train_score(preds, labels.to(torch.int))\n",
    "        # print(preds, labels.to(torch.int))\n",
    "        # print(preds.argmax(axis=1), labels.to(torch.int).argmax(axis=1))\n",
    "        self.log(\"train_f1_score\", self.train_score)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss  \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "        imgs, labels = batch\n",
    "        labels = np.squeeze(labels)\n",
    "        preds = np.squeeze(self.model(imgs))\n",
    "        if len(preds.shape) < 2:\n",
    "            preds = preds.view(1, -1)\n",
    "            labels = labels.view(1, -1)\n",
    "        # print(torch.round(preds, decimals=2))\n",
    "        # print(torch.round(labels, decimals=2))\n",
    "        if False:\n",
    "            preds = preds.argmax(axis=1)\n",
    "            labels = labels.to(torch.int).argmax(axis=1)\n",
    "        # print(preds)\n",
    "        # print(labels)\n",
    "        self.val_acc(preds, labels)\n",
    "        # print(preds)\n",
    "        # print(labels)\n",
    "        # print(preds.shape, labels.shape)\n",
    "        # print(preds.argmax(axis=1), labels.to(torch.int).argmax(axis=1))\n",
    "        self.val_score(preds, labels.to(torch.int))\n",
    "        self.log(\"val_f1_score\", self.val_score)\n",
    "        self.log(\"val_acc\", self.val_acc)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        preds = np.squeeze(self(batch))\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model_class=ResNet1D, train_continue = True, save_name=None, pretrained_filename=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    if save_name is None:\n",
    "        save_name = model_name\n",
    "\n",
    "    curr_model_save_path = os.path.join(CHECKPOINT_PATH, save_name)\n",
    "    trainer = pl.Trainer(\n",
    "        check_val_every_n_epoch=2,\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models\n",
    "        # We run on a single GPU (if possible)\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        # How many epochs to train for if no patience is set\n",
    "        max_epochs=10,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                mode=\"max\", monitor=\"val_f1_score\", save_top_k=2,\n",
    "            ), \n",
    "            EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=15),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],  # Log learning rate every epoch\n",
    "    ) \n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = Lightning_ResNet1D.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        if(pretrained_filename!=\"\"):\n",
    "            print(\"FAILED TO LOAD A MODEL\")\n",
    "            return\n",
    "        # L.seed_everything(42)  # To be reproducable\n",
    "        if train_continue:\n",
    "            default_root_dir = os.path.join(CHECKPOINT_PATH, save_name) \n",
    "            default_root_dir = os.path.join(default_root_dir, \"lightning_logs\")\n",
    "            continue_path = os.path.join(default_root_dir, os.listdir(default_root_dir)[-1])\n",
    "            continue_path = os.path.join(continue_path, \"checkpoints\")\n",
    "            continue_path = os.path.join(continue_path, os.listdir(continue_path)[-1])\n",
    "\n",
    "\n",
    "        model = Lightning_ResNet1D(model_name=model_name, model_class=model_class, **kwargs)\n",
    "        if train_continue:\n",
    "            trainer.fit(model, train_loader, val_loader, ckpt_path=continue_path)\n",
    "        else:\n",
    "            trainer.fit(model, train_loader, val_loader,)\n",
    "        model = Lightning_ResNet1D.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )  # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.validate(model, dataloaders=val_loader, verbose=False)\n",
    "    result = {\"val_acc\": val_result[0][\"val_acc\"], \"val_f1_score\": val_result[0][\"val_f1_score\"]}\n",
    "    \n",
    "    \n",
    "    return model, result, curr_model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params | In sizes     | Out sizes\n",
      "------------------------------------------------------------------------------\n",
      "0 | model       | ResNet1D           | 653 K  | [1, 12, 500] | [1, 7]   \n",
      "1 | loss_module | BCELoss            | 0      | ?            | ?        \n",
      "2 | train_score | MultilabelF1Score  | 0      | ?            | ?        \n",
      "3 | val_score   | MultilabelF1Score  | 0      | ?            | ?        \n",
      "4 | test_score  | MultilabelF1Score  | 0      | ?            | ?        \n",
      "5 | val_acc     | MultilabelAccuracy | 0      | ?            | ?        \n",
      "6 | train_acc   | MultilabelAccuracy | 0      | ?            | ?        \n",
      "------------------------------------------------------------------------------\n",
      "653 K     Trainable params\n",
      "0         Non-trainable params\n",
      "653 K     Total params\n",
      "2.614     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbb0b797c5e400389ee0c521b45f2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4564fac7dfb54d0ab3aec53d8343cfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#удалить аргумент pretrained filename, для тренировки заново\n",
    "resnet_model, resnet_results, curr_model_save_path = train_model(\n",
    "    train_continue=False,\n",
    "    # pretrained_filename=\"saved_models/ConvNets/ResNet1D_denoising_level2/lightning_logs/version_3/checkpoints/epoch=63-step=23104.ckpt\",     \n",
    "    model_name=\"ResNet1D_v2\",\n",
    "    model_class=ResNet1D,\n",
    "    save_name=\"ResNet1D_denoising_level2\", \n",
    "    model_hparams={\"n_classes\": 7, \"base_filters\": 16, \"kernel_size\": 16, \"stride\": 2, \"groups\": 1, \"n_block\": 12, \"in_channels\": 12},\n",
    "    optimizer_name=\"Adam\",\n",
    "    optimizer_hparams={\"lr\": 0.0001,  \"weight_decay\": 1e-4},\n",
    "    task=\"MULTILABEL\"\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_acc': 0.8630075454711914, 'val_f1_score': 0.4969629943370819}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import mode\n",
    "\n",
    "resnet_model.eval()\n",
    "\n",
    "def predict(beat_filename):\n",
    "    test_beat = np.load(beat_filename).astype(np.float32) \n",
    "    test_beat = test_beat.reshape((1,12,-1))\n",
    "    res = resnet_model(torch.from_numpy(test_beat).float())\n",
    "    return res.detach().numpy()\n",
    "\n",
    "def run_test_predicts(curr_model_save_path, treshhold=0.5):\n",
    "    df_og = pd.read_csv(\"../task_final/train/train_meta.csv\")\n",
    "    test_annotations = pd.read_csv(\"./val_annotations.csv\")\n",
    "    preds = {}\n",
    "    for name in tqdm(test_annotations[\"new_name\"]):\n",
    "        record_name = name[:name.rfind(\"_\")]\n",
    "        if record_name not in preds:\n",
    "            preds[record_name] = []\n",
    "        preds[record_name].append(predict(\"./transformed_train/\"+name+\".npy\"))\n",
    "\n",
    "    preds_median = {k: ((np.array(v).mean(axis=0) > treshhold) * 1)[0].tolist() for k,v in preds.items()}\n",
    "    df_og[\"predict\"] = df_og[\"record_name\"].map(preds_median)\n",
    "    return df_og\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee862fafa86e4f9daf680dd4e97db1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_res = run_test_predicts(curr_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = test_res[~test_res.predict.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.predict.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = pd.DataFrame(test_res.predict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotations = pd.read_csv(\"../task_final/train/train_gts_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res2 = test_res.merge(test_annotations, on=\"record_name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "перегородочный F1 0.11881188118811878\n",
      "передний F1 0.4\n",
      "боковой F1 0.0\n",
      "передне-боковой F1 0.46153846153846156\n",
      "передне-перегородочный F1 0.0\n",
      "нижний F1 0.5490196078431373\n",
      "норма F1 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TOTAL', 0.21848142150995967)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "summs = 0\n",
    "for i, column in enumerate([\"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\", \"норма\"]):\n",
    "    print(column, \"F1\", f1_score(test_res2[column], predicts[i]))\n",
    "    summs += f1_score(test_res2[column], predicts[i])\n",
    "\"TOTAL\", summs / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_name                  07304_hr_n1918054_hr_n518054_hr_n814491_hr_n11...\n",
       "target                                                                16460\n",
       "перегородочный                                                          849\n",
       "передний                                                                961\n",
       "боковой                                                                1000\n",
       "передне-боковой                                                         916\n",
       "передне-перегородочный                                                 1013\n",
       "нижний                                                                 1730\n",
       "норма                                                                   804\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./train_annotations.csv\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_res[~test_res.predict.isna()][\"predict\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict тренировочных данных и тестовых данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import mode\n",
    "\n",
    "resnet_model.eval()\n",
    "\n",
    "def predict(beat_filename):\n",
    "    test_beat = np.load(beat_filename).astype(np.float32) \n",
    "    test_beat = test_beat.reshape((1,12,-1))\n",
    "    res = resnet_model(torch.from_numpy(test_beat).float())\n",
    "    return res.detach().numpy()\n",
    "\n",
    "def run_test_predicts(curr_model_save_path):\n",
    "    df_og = pd.read_csv(\"../task_final/test/test_meta.csv\")\n",
    "    test_annotations = pd.read_csv(\"./transformed_test_df.csv\")\n",
    "    preds = {}\n",
    "    for name in tqdm(test_annotations[\"new_name\"].values):\n",
    "        record_name = name[:name.rfind(\"_\")]\n",
    "        if record_name not in preds:\n",
    "            preds[record_name] = []\n",
    "        preds[record_name].append(predict(\"./transformed_test/\"+name+\".npy\"))\n",
    "    \n",
    "    print(np.round(np.array(preds[\"00127_hr\"]).mean(axis=0), 2))\n",
    "    preds_median = {k: np.array(v).mean(axis=0) > 0.5 for k,v in preds.items()}\n",
    "    df_og[\"predict\"] = df_og[\"record_name\"].map(preds_median)\n",
    "\n",
    "    preds_median = {k: np.array(v).mean(axis=0).argmax() for k,v in preds.items()}\n",
    "    df_og[\"target\"] = df_og[\"record_name\"].map(preds_median)\n",
    "    \n",
    "    save_path = os.path.join(curr_model_save_path, \"predicted_test.csv\")\n",
    "    df_og.to_csv(save_path)\n",
    "    df_og.to_csv(\"./predicted_test.csv\")\n",
    "    print(\"Соотношение предсказанных классов:\")\n",
    "    display(df_og['target'].apply(round).value_counts(normalize=True))\n",
    "    return df_og\n",
    "\n",
    "def preds_train_df(curr_model_save_path):\n",
    "    df = pd.read_csv(\"./transformed_df.csv\")\n",
    "    preds = {}\n",
    "    for name in tqdm(df[\"new_name\"].values):\n",
    "        record_name = name[:name.rfind(\"_\")]\n",
    "        if record_name not in preds:\n",
    "            preds[record_name] = []\n",
    "        preds[record_name].append(predict(\"./transformed_train/\"+name+\".npy\"))\n",
    "    \n",
    "    df_og = pd.read_csv(\"../task_final/train/train_gts_final.csv\")\n",
    "    \n",
    "    preds_median = {k:np.median(np.array(v)) for k,v in preds.items()}\n",
    "    df_og['predict'] = df_og[\"record_name\"].map(preds_median)\n",
    "    save_path = os.path.join(curr_model_save_path, \"predicted_train.csv\")\n",
    "    df_og.to_csv(save_path)\n",
    "    df_og.to_csv(\"./predicted_train.csv\")\n",
    "    print(\"Соотношение предсказанных классов:\")\n",
    "    display(df_og['predict'].apply(round).value_counts(normalize=True))\n",
    "    return preds, df_og\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_submit = pd.read_csv(\"old_submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3945b211894452c8bb4cde416dfb8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39 0.16 0.03 0.13 0.37 0.06 0.04]]\n",
      "Соотношение предсказанных классов:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.603563\n",
       "5    0.236080\n",
       "4    0.057906\n",
       "1    0.044543\n",
       "3    0.042316\n",
       "2    0.015590\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (7,) into shape (6,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m one_hot_enc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m449\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m449\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mone_hot_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m test_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (7,) into shape (6,)"
     ]
    }
   ],
   "source": [
    "test_res = run_test_predicts(curr_model_save_path)\n",
    "# preds, df_og = preds_train_df(curr_model_save_path)\n",
    "\n",
    "one_hot_enc = np.zeros((449, 6))\n",
    "for i in range(449):\n",
    "    one_hot_enc[i] = test_res[\"predict\"].iloc[i] * 1 + [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_df = pd.DataFrame(one_hot_enc, columns=[\"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\"])\n",
    "onehot_df[\"record_name\"] = test_res[\"record_name\"]\n",
    "subm_df = test_res.merge(onehot_df, on=\"record_name\", how=\"left\")\n",
    "subm_df = subm_df.merge(old_submit, on=\"record_name\", how=\"left\")\n",
    "subm_df[\"норма\"] = subm_df[\"myocard\"].map(lambda x: 0 if x == 1 else 1)\n",
    "for column in [\"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\"]:\n",
    "    subm_df[column] = subm_df['myocard'] & subm_df[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df[[\"record_name\", \"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\", \"норма\"]].to_csv(\"submit.csv\", index=False)\n",
    "subm_df[[\"record_name\", \"перегородочный\", \"передний\", \"боковой\", \"передне-боковой\", \n",
    "                             \"передне-перегородочный\", \"нижний\", \"норма\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание всех тренировочных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean = {k:(sum(v)/len(v)) for k,v in preds.items()}\n",
    "preds_median = {k:np.median(np.array(v)) for k,v in preds.items()}\n",
    "preds_max= {k:np.argmax(np.array(v)) for k,v in preds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(preds, df_og):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "\n",
    "    for k, v in preds.items():\n",
    "        predicted_class = round(v)\n",
    "        actual_class = df_og[df_og[\"record_name\"] == k][\"myocard\"].values[0]\n",
    "        \n",
    "        if actual_class == 1 and predicted_class == 1:\n",
    "            tp += 1\n",
    "        elif actual_class == 0 and predicted_class == 0:\n",
    "            tn += 1\n",
    "        elif actual_class == 0 and predicted_class == 1:\n",
    "            fp += 1\n",
    "        elif actual_class == 1 and predicted_class == 0:\n",
    "            fn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    print(\"accuracy\", (tp + tn)/len(preds))\n",
    "    print(\"true positive rate, recall\", (tp/(tp+fn)))\n",
    "    print(\"true negative rate\", tnr)\n",
    "    print(\"f1_score\", 2*(precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(preds_mean, df_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(preds_median, df_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(preds_max, df_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val lb\n",
    "# 0.6321044564247131 0.7230883444691908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
