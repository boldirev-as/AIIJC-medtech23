{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import torchmetrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from torchmetrics.classification.accuracy import BinaryAccuracy\n",
    "import lightning as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.lr_monitor import LearningRateMonitor\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from wavelets_cnn import ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\n",
    "    \"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/ConvNets\")\n",
    "\n",
    "\n",
    "# Function for setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted beats, as explained in\n",
    "Section III-A, are used as inputs. Here, all convolution layers\n",
    "are applying 1-D convolution through time and each have 32\n",
    "kernels of size 5. We also use max pooling of size 6 and stride\n",
    "2 in all pooling layers. The predictor network consists of five\n",
    "residual blocks followed by two fully-connected layers with\n",
    "32 neurons each and a softmax layer to predict output class probabilities. Each residual block contains two convolutional\n",
    "layers, two ReLU nonlinearities [19], a residual skip connec-\n",
    "tion [20], and a pooling layer. In total, the resulting network\n",
    "is a deep network consisting of 13 weight layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "\n",
    "    def forward(self , x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + residual  # Residual connection\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, sequence_len, n_classes, n_blocks=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5,stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(32),\n",
    "            ResidualBlock(32),\n",
    "            ResidualBlock(32),\n",
    "            ResidualBlock(32),\n",
    "            ResidualBlock(32)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(sequence_len, 32), # 20\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.residual_blocks(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten to [batch_size, channels * sequence_length]\n",
    "\n",
    "        return self.classifier(out)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 375, 375)\n",
      "test beat shape (1, 12, 375, 375)\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.2583, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_beat = np.load('./transformed_train/00034_hr.npy')\n",
    "ecg_lead = 9\n",
    "segment_num = 1\n",
    "print(test_beat.shape)\n",
    "model = ResNet(num_classes=1)\n",
    "#model = Baseline(test_beat.shape[1]-128, 1)\n",
    "\n",
    "\n",
    "test_beat = test_beat.reshape((1,12,375, -1))\n",
    "test_y = torch.tensor([[1.]])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(\"test beat shape\", test_beat.shape)\n",
    "res= model(torch.from_numpy(test_beat).float())\n",
    "print(res.shape)\n",
    "criterion(res, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetECG(Dataset):\n",
    "    def __init__(self, annotations_file, signals_dir):\n",
    "        \"\"\"\n",
    "        annotantions_file - path to the annotations dataframe. \n",
    "                            First column should be name of the record, second - strat_fold then labels \n",
    "        \n",
    "        signals_dir - path to the directory with transformed signals\n",
    "        \"\"\"\n",
    "        self.signals_labels = pd.read_csv(annotations_file)\n",
    "        self.signals_dir = signals_dir \n",
    "        self.ecg_lead = 9\n",
    "        self.segment_num = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signals_path = os.path.join(self.signals_dir, self.signals_labels.iloc[idx, 0]+ \".npy\")\n",
    "        signal = np.load(signals_path).astype(np.float32)        \n",
    "\n",
    "        # iloc[idx, 2:] 2 is because first column is a record name\n",
    "        labels = torch.from_numpy(self.signals_labels.iloc[idx, 2:].values.astype(int)).float()\n",
    "        return signal, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetECG(\"./train_annotations.csv\", \"transformed_train\")\n",
    "val_dataset = DatasetECG(\"./val_annotations.csv\", \"transformed_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitBaseline(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.train_score = F1Score()\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        self.val_score = F1Score()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0),12, -1)\n",
    "        pred = self.model(x)\n",
    "        loss = self.criterion(pred, y)\n",
    "        self.train_score(pred,y.to(torch.int))\n",
    "        self.log(\"f1_score\", self.train_score)\n",
    "        self.log(\"loss\", loss, prog_bar=True, logger=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0),12, -1)\n",
    "        pred = self.model(x)\n",
    "        val_loss = self.criterion(pred, y)\n",
    "        self.val_score(pred,y.to(torch.int))\n",
    "        self.log(\"val_f1_score\", self.val_score)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если один батч - лосс converges в 0, если 5 - в пизде какой то. Если менять lr сильно ниче не меняется, если поменять sequence_len в препроцессинге(transforming.ipynb, BEAT_LENGTH) тоже особо не меняется. Когда sequence_len была 1400 и 5 батчей лосс конвержился в 19 хз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWT_ResNet(L.LightningModule):\n",
    "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams - Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
    "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = ResNet(**model_hparams)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.BCEWithLogitsLoss()\n",
    "        self.train_score = BinaryF1Score()\n",
    "        self.val_score = BinaryF1Score()\n",
    "        self.test_score = BinaryF1Score()\n",
    "        self.val_acc = BinaryAccuracy()\n",
    "        self.train_acc = BinaryAccuracy()\n",
    "        # Example input for visualizing the graph in Tensorboard\n",
    "        self.example_input_array = torch.zeros((1, 12, 32, 32), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # Forward function that is run when visualizing the graph\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We will support Adam or SGD as optimizers.\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            # AdamW is Adam with a correct implementation of weight decay (see here\n",
    "            # for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
    "            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        elif self.hparams.optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n",
    "\n",
    "        # We will reduce the learning rate by 0.1 every milestone\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[65, 115, 150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        imgs, labels = batch\n",
    "        labels = np.squeeze(labels)\n",
    "        preds = np.squeeze(self.model(imgs))\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.train_acc(preds, np.squeeze(labels).to(torch.int))\n",
    "\n",
    "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "        self.train_score(preds, labels.to(torch.int))\n",
    "        self.log(\"train_f1_score\", self.train_score)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        labels = np.squeeze(labels)\n",
    "        preds = np.squeeze(self.model(imgs))\n",
    "        self.val_acc(preds, labels.to(torch.int))\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        self.val_score(preds, labels.to(torch.int))\n",
    "        self.log(\"val_f1_score\", self.val_score)\n",
    "        self.log(\"val_acc\", self.val_acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs).argmax(dim=-1)\n",
    "        acc = (labels == preds).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
    "        self.test_score(preds, labels.to(torch.int))\n",
    "        self.log(\"val_f1_score_test\", self.test_score)\n",
    "        self.log(\"test_acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, save_name=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    if save_name is None:\n",
    "        save_name = model_name\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = L.Trainer(\n",
    "        check_val_every_n_epoch=2,\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models\n",
    "        # We run on a single GPU (if possible)\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        # How many epochs to train for if no patience is set\n",
    "        max_epochs=180,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                mode=\"max\", monitor=\"val_f1_score\", save_top_k=2,\n",
    "            ),  \n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],  # Log learning rate every epoch\n",
    "    )  # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        trainer.fit(model, train_loader, val_loader, ckpt_path=pretrained_filename)\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = CWT_ResNet.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        L.seed_everything(42)  # To be reproducable\n",
    "        model = CWT_ResNet(model_name=model_name, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = CWT_ResNet.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )  # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    result = {\"val\": val_result[0][\"test_acc\"], \"val_f1_score\": val_result[0][\"val_f1_score_test\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | In sizes        | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | model       | ResNet            | 273 K  | [1, 12, 32, 32] | [1, 1]   \n",
      "1 | loss_module | BCEWithLogitsLoss | 0      | ?               | ?        \n",
      "2 | train_score | BinaryF1Score     | 0      | ?               | ?        \n",
      "3 | val_score   | BinaryF1Score     | 0      | ?               | ?        \n",
      "4 | test_score  | BinaryF1Score     | 0      | ?               | ?        \n",
      "5 | val_acc     | BinaryAccuracy    | 0      | ?               | ?        \n",
      "6 | train_acc   | BinaryAccuracy    | 0      | ?               | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "273 K     Trainable params\n",
      "0         Non-trainable params\n",
      "273 K     Total params\n",
      "1.092     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957bb371d4c84e728fb1498a6470bd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8e11d2e9fc41589632e41a169ef408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7254bdaf80144ef87131d980798ca1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba2a75246c644d8a9d368f31300ad7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc462c0bae264f7e9143174780a8fb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fa1c0f78f24372a06bd15f7fb04e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1a5b0f80544054a040757947645a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc4652f03ce4bd7bf70013b4a54f14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9198f1b08fd542618ab2b4b2d10803ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456c7998201749dea46bba0410d90902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f3164dc8ac44deb2c8dd8598e94afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e26d82a0d2481eb9194090767c3e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d201ae67acd5433d9af1c71006793499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4083af71f57493a958b70df0bec61bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf29ae90b134052a033126724ea528f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a51a6e60be4c09a2bff3b24a5d06c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301cba0c13ac48c5a0a046690cdffa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c26b984d68148c5a32c6083f33c5e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a42b8a2dc84a23a47db852e04ba0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e52cb77d84248f787504d77def3aead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f336a44f94e45c88338e8804fcc48bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c91d20c94348b0972fb2a3ba928fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5da3e5384447cfa57002b0cf8925a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc000dc60d904816b4f3993afd415ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Predictions and targets are expected to have the same shape, but got torch.Size([16]) and torch.Size([16, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m resnet_model, resnet_results \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcwt_resNet_v3_lower_c_less_blocks\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcwt_resNet_v4\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_hparams\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnum_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mc_hidden\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m16\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m64\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mnum_blocks\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mact_fn_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSGD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     optimizer_hparams\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.0001\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.9\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1e-4\u001b[39;49m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "\u001b[1;32m/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, save_name, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     model \u001b[39m=\u001b[39m CWT_ResNet\u001b[39m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         trainer\u001b[39m.\u001b[39mcheckpoint_callback\u001b[39m.\u001b[39mbest_model_path\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     )  \u001b[39m# Load best checkpoint after training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Test best model on validation and test set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m val_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtest(model, dataloaders\u001b[39m=\u001b[39;49mval_loader, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m result \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m: val_result[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mval_f1_score\u001b[39m\u001b[39m\"\u001b[39m: val_result[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mval_f1_score_test\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model, result\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:742\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    741\u001b[0m _verify_strategy_supports_compile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 742\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    743\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    744\u001b[0m )\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:785\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    782\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    783\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m )\n\u001b[0;32m--> 785\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    786\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    787\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1016\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1018\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    295\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    296\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtest_step_context():\n\u001b[1;32m    402\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb Cell 14\u001b[0m in \u001b[0;36mCWT_ResNet.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m acc \u001b[39m=\u001b[39m (labels \u001b[39m==\u001b[39m preds)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# By default logs it per epoch (weighted average over batches), and returns it afterwards\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_score(preds, labels\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mint))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_f1_score_test\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X16sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m\"\u001b[39m, acc)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/metric.py:245\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_full_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_reduce_state_update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/metric.py:309\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    310\u001b[0m batch_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute()\n\u001b[1;32m    312\u001b[0m \u001b[39m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/metric.py:405\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n\u001b[1;32m    398\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m device corresponds to the device of the input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_cpu:\n\u001b[1;32m    408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/metric.py:395\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[1;32m    394\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m         update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    396\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    397\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/classification/stat_scores.py:172\u001b[0m, in \u001b[0;36mBinaryStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m\"\"\"Update state with predictions and targets.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m    preds: Tensor with predictions\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    target: Tensor with true labels\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 172\u001b[0m     _binary_stat_scores_tensor_validation(preds, target, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultidim_average, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index)\n\u001b[1;32m    173\u001b[0m preds, target \u001b[39m=\u001b[39m _binary_stat_scores_format(preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index)\n\u001b[1;32m    174\u001b[0m tp, fp, tn, fn \u001b[39m=\u001b[39m _binary_stat_scores_update(preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultidim_average)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/functional/classification/stat_scores.py:62\u001b[0m, in \u001b[0;36m_binary_stat_scores_tensor_validation\u001b[0;34m(preds, target, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m\"\"\"Validate tensor input.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m- tensors have to be of same shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m- if ``multidim_average`` is set to ``samplewise`` preds tensor needs to be atleast 2 dimensional\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# Check that they have same shape\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m _check_same_shape(preds, target)\n\u001b[1;32m     64\u001b[0m \u001b[39m# Check that target only contains [0,1] values or value in ignore_index\u001b[39;00m\n\u001b[1;32m     65\u001b[0m unique_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(target)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:35\u001b[0m, in \u001b[0;36m_check_same_shape\u001b[0;34m(preds, target)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m\"\"\"Check that predictions and target have the same shape, else raise error.\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m target\u001b[39m.\u001b[39mshape:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     36\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and targets are expected to have the same shape, but got \u001b[39m\u001b[39m{\u001b[39;00mpreds\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Predictions and targets are expected to have the same shape, but got torch.Size([16]) and torch.Size([16, 1])."
     ]
    }
   ],
   "source": [
    "resnet_model, resnet_results = train_model(\n",
    "    model_name=\"cwt_resNet_v3_lower_c_less_blocks\",\n",
    "    save_name=\"cwt_resNet_v4\", \n",
    "    model_hparams={\"num_classes\": 1, \"c_hidden\": [16, 32, 64], \"num_blocks\": [3, 3, 3], \"act_fn_name\": \"relu\"},\n",
    "    optimizer_name=\"SGD\",\n",
    "    optimizer_hparams={\"lr\": 0.0001, \"momentum\": 0.9, \"weight_decay\": 1e-4},\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "# model\n",
    "autoencoder = LitBaseline(Baseline(sequence_len=768-128,n_classes=1,n_blocks=5))\n",
    "\n",
    "checkpoint_callback =  ModelCheckpoint(dirpath=\"./lightning_logs/best_run1/\", save_top_k=2, monitor=\"val_f1_score\")\n",
    "# train model\n",
    "# trainer = pl.Trainer(max_epochs=100, check_val_every_n_epoch=5,enable_checkpointing=True,\n",
    "#                      callbacks=[EarlyStopping(monitor=\"val_f1_score\", mode='max', patience=15), checkpoint_callback])\n",
    "# trainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mvalidate(autoencoder, val_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.validate(autoencoder, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lightning_logs/version_52/metrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m overfit_logs \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./lightning_logs/version_52/metrics.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(overfit_logs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/biba/Documents/programming/aiijc/version_1/model_training.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df2\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/mnt/sda5/conda/envs/open-mmlab/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lightning_logs/version_52/metrics.csv'"
     ]
    }
   ],
   "source": [
    "overfit_logs = \"./lightning_logs/version_52/metrics.csv\"\n",
    "df2 = pd.read_csv(overfit_logs)\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
